# Harbor AI - robots.txt
# Emergency Recovery Configuration - 2026-02-02

User-agent: *
Allow: /

# Sitemap location - Points to dynamic backend sitemap
Sitemap: https://harborml.com/api/seo/sitemap-index.xml

# Allow core content paths
Allow: /tools/
Allow: /guides/
Allow: /compare/
Allow: /explore/

# Block excessive programmatic crawling until trust is established
# Disallow: /tools/*/* (Commented out to allow specific sitemap-lego URLs, but careful)

# Block authenticated/app routes
Disallow: /app/
Disallow: /auth/
Disallow: /onboarding/
Disallow: /settings/
Disallow: /api/

# Block utility pages
Disallow: /404
Disallow: /500

# Allow static assets
Allow: /assets/
Allow: /*.css
Allow: /*.js
Allow: /*.svg
Allow: /*.png
Allow: /*.jpg
Allow: /*.webp

# Crawl delay (optional, be nice to servers)
Crawl-delay: 1

# Google-specific
User-agent: Googlebot
Allow: /

# Bing-specific
User-agent: Bingbot
Allow: /

# Block bad bots
User-agent: AhrefsBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /
